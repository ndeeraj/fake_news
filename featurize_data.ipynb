{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac56dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from readability import Readability\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f356f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pd.read_csv('data/True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e6ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "real[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('data/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df77af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f594181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([real, fake], ignore_index=True).drop(['title','subject','date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f42e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     44898\n",
       "label    44898\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648b2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['text'].astype(str)\n",
    "    df['clean_text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False) #Removing urls\n",
    "    #df['clean_text'] = df['clean_text'].str.replace('[^' + string.printable + ']', '')\n",
    "    df['clean_text'] = df['clean_text'].str.replace('[^A-Za-z0-9]+', ' ') # Removing Punctuations, Numbers, and Special Character \n",
    "    df['clean_text'] = df['clean_text'].str.replace('\\s+', ' ')\n",
    "    df['clean_text'] = df['clean_text'].map(lambda x: x if type(x)!=str else x.strip().lower()) #lowercase\n",
    "    df['clean_text'].dropna(inplace=True) #drop NaN values\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dac253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deera\\AppData\\Local\\Temp\\ipykernel_32276\\3480421439.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False) #Removing urls\n",
      "C:\\Users\\deera\\AppData\\Local\\Temp\\ipykernel_32276\\3480421439.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_text'] = df['clean_text'].str.replace('[^A-Za-z0-9]+', ' ') # Removing Punctuations, Numbers, and Special Character\n",
      "C:\\Users\\deera\\AppData\\Local\\Temp\\ipykernel_32276\\3480421439.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_text'] = df['clean_text'].str.replace('\\s+', ' ')\n"
     ]
    }
   ],
   "source": [
    "clean_df = preprocess(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ed456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [word for word in token_text if word not in stopwords]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e62eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['clean_text'] = clean_df.clean_text.apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2533ff32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          44898\n",
       "label         44898\n",
       "clean_text    44898\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5b2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.0'\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.0.1+cpu)\n",
      "    Python  3.10.11 (you have 3.10.4)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "block_size = 350\n",
    "\n",
    "def analyze_sentiment(text_to_analyze):\n",
    "    words = text_to_analyze.split()\n",
    "    txt_blocks = []\n",
    "    if len(words) > block_size:\n",
    "        while len(words) > block_size:\n",
    "            txt_blocks.append(\" \".join(words[:block_size]))\n",
    "            words = words[block_size:]\n",
    "        # last block - just adding it if it has decent amount of tokens\n",
    "        if len(words) > 250:\n",
    "            txt_blocks.append(\" \".join(words))\n",
    "    else:\n",
    "        txt_blocks.append(\" \".join(words))\n",
    "    \n",
    "    first_sent_res = None\n",
    "    overall_sent = 0\n",
    "    for i, blk in enumerate(txt_blocks):\n",
    "        try:\n",
    "            sentiment_res = sentiment_pipeline([blk])\n",
    "        except:\n",
    "            print(len(blk.split()))\n",
    "            continue\n",
    "    \n",
    "        sent = sentiment_res[0].get(\"label\").lower()\n",
    "        if sent == \"positive\":\n",
    "            overall_sent += 1\n",
    "            if first_sent_res is None:\n",
    "                first_sent_res = 1\n",
    "        elif sent == \"negative\":\n",
    "            overall_sent -= 1\n",
    "            if first_sent_res is None:\n",
    "                first_sent_res = 0\n",
    "        else:\n",
    "            raise RuntimeError(f\"unknown label from sentiment analyzer: {sent}\")\n",
    "    \n",
    "    if overall_sent > 0:\n",
    "        return 1\n",
    "    elif overall_sent < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return first_sent_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e67b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feat(input_df):\n",
    "    out_df = pd.DataFrame()\n",
    "    skip_pos_tags = ['``',':', '.', \"''\", '$', 'NNPS', 'WDT', 'RBS', 'WP', 'POS', 'UH', 'WRB', 'EX', 'PRP$', 'TO', 'SYM', 'PDT', 'RP', 'CC', 'FW', 'PRP', 'WP$', 'NNP', 'DT', 'RBR']\n",
    "    skip_pos_tags_dict = {}\n",
    "    for tag in skip_pos_tags:\n",
    "        skip_pos_tags_dict[tag] = True\n",
    "    \n",
    "    for i in range(len(input_df)):\n",
    "        print(f\"{i} / {len(input_df)}\", end='\\r')\n",
    "        clean_txt = input_df.loc[i, \"clean_text\"]\n",
    "        out_df.loc[i, \"text\"] = clean_txt\n",
    "        out_df.loc[i, \"label\"] = input_df.loc[i, \"label\"]\n",
    "        out_df.loc[i, \"token_length\"] = len(clean_txt.split())\n",
    "        \n",
    "        word_pos_tags = nltk.pos_tag(word_tokenize(clean_txt))\n",
    "        for word in word_pos_tags:\n",
    "            tag = \"WTAG_\" + word[1]\n",
    "            if skip_pos_tags_dict.get(word[1]):\n",
    "                continue\n",
    "            try:\n",
    "                if np.isnan(out_df.loc[i, tag]): \n",
    "                    out_df.loc[i, tag] = 1\n",
    "                else:\n",
    "                    out_df.loc[i, tag] += 1\n",
    "            except KeyError:\n",
    "                out_df.loc[i, tag] = 1\n",
    "        \n",
    "        if len(clean_txt.split()) > 100:\n",
    "            r = Readability(clean_txt)\n",
    "            out_df.loc[i, \"fk_score\"] = r.flesch_kincaid().score\n",
    "            out_df.loc[i, \"flesh_score\"] = r.flesch().score\n",
    "            out_df.loc[i, \"ari_score\"] = r.ari().score\n",
    "            out_df.loc[i, \"cl_score\"] = r.coleman_liau().score\n",
    "            out_df.loc[i, \"gf_score\"] = r.gunning_fog().score\n",
    "        else:\n",
    "            out_df.loc[i, \"fk_score\"] = \\\n",
    "            out_df.loc[i, \"flesh_score\"] = \\\n",
    "            out_df.loc[i, \"ari_score\"] = \\\n",
    "            out_df.loc[i, \"cl_score\"] = \\\n",
    "            out_df.loc[i, \"gf_score\"] = 0\n",
    "        \n",
    "        out_df.loc[i, \"sentiment\"] = analyze_sentiment(str(clean_txt))\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd0c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21487 / 44898\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35088 / 44898\n",
      "35096 / 44898\n",
      "35004 / 44898\n",
      "35088 / 44898\n",
      "35005 / 44898\n",
      "35018 / 44898\n",
      "35046 / 44898\n",
      "35068 / 44898\n",
      "35049 / 44898\n",
      "35093 / 44898\n",
      "34613 / 44898\n",
      "35022 / 44898\n",
      "35062 / 44898\n",
      "35045 / 44898\n",
      "35078 / 44898\n",
      "35003 / 44898\n",
      "35081 / 44898\n",
      "35093 / 44898\n",
      "35096 / 44898\n",
      "35009 / 44898\n",
      "35088 / 44898\n",
      "35036 / 44898\n",
      "35007 / 44898\n",
      "35013 / 44898\n",
      "35062 / 44898\n",
      "35074 / 44898\n",
      "35003 / 44898\n",
      "35020 / 44898\n",
      "35021 / 44898\n",
      "35059 / 44898\n",
      "35073 / 44898\n",
      "35007 / 44898\n",
      "35016 / 44898\n",
      "35042 / 44898\n",
      "35045 / 44898\n",
      "32946 / 44898\n",
      "35056 / 44898\n",
      "35074 / 44898\n",
      "35023 / 44898\n",
      "35095 / 44898\n",
      "35041 / 44898\n",
      "35076 / 44898\n",
      "35099 / 44898\n",
      "35019 / 44898\n",
      "337\n",
      "35030 / 44898\n",
      "31533 / 44898\n",
      "33138 / 44898\n",
      "32245 / 44898\n",
      "34274 / 44898\n",
      "35006 / 44898\n",
      "35013 / 44898\n",
      "35016 / 44898\n",
      "35023 / 44898\n",
      "35033 / 44898\n",
      "35040 / 44898\n",
      "35079 / 44898\n",
      "35085 / 44898\n",
      "35098 / 44898\n",
      "35002 / 44898\n",
      "34724 / 44898\n",
      "35033 / 44898\n",
      "35036 / 44898\n",
      "35051 / 44898\n",
      "35067 / 44898\n",
      "35070 / 44898\n",
      "35082 / 44898\n",
      "35093 / 44898\n",
      "34198 / 44898\n",
      "35093 / 44898\n",
      "35094 / 44898\n",
      "35087 / 44898\n",
      "34733 / 44898\n",
      "35059 / 44898\n",
      "35017 / 44898\n",
      "35025 / 44898\n",
      "35077 / 44898\n",
      "350\n",
      "35039 / 44898\n",
      "35031 / 44898\n",
      "31641 / 44898\n",
      "32752 / 44898\n",
      "35071 / 44898\n",
      "35063 / 44898\n",
      "35037 / 44898\n",
      "35026 / 44898\n",
      "35014 / 44898\n",
      "35058 / 44898\n",
      "32483 / 44898\n",
      "34342 / 44898\n",
      "35019 / 44898\n",
      "35044 / 44898\n",
      "350\n",
      "34246 / 44898\n",
      "35019 / 44898\n",
      "35054 / 44898\n",
      "35009 / 44898\n",
      "35057 / 44898\n",
      "35061 / 44898\n",
      "26197 / 44898\n",
      "35028 / 44898\n",
      "35036 / 44898\n",
      "35015 / 44898\n",
      "35045 / 44898\n",
      "35073 / 44898\n",
      "35076 / 44898\n",
      "35048 / 44898\n",
      "35034 / 44898\n",
      "44897 / 44898\r"
     ]
    }
   ],
   "source": [
    "feat_df = generate_feat(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "683f9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.to_csv('data/feat_new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
